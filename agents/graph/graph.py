import asyncio
from langgraph.graph import StateGraph, END, START
import os
from dotenv import load_dotenv
from typing import Optional, Dict, Any, List
from langchain.agents.factory import create_agent
from langchain_mcp_adapters.client import MultiServerMCPClient
import requests
from pretty_print import debug_all

SERVER_HOST = '0.0.0.0'
SERVER_PORT = 8001
SERVER_URL = f'http://{SERVER_HOST}:{SERVER_PORT}'
MCP_SERVERS = {
        "my_server": {
            "url": SERVER_URL+"/mcp",
            "transport": "http",
        },
    }

# ============================================================
# Environment & LLM
# ============================================================

load_dotenv()

# Model provider selection: "openai" or "ollama" (free)
MODEL_PROVIDER = os.getenv("MODEL_PROVIDER", "openai").lower()

if MODEL_PROVIDER == "openai":
    from langchain_openai import ChatOpenAI
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY is not set in .env file")
    # Model configuration - can be overridden via environment variable
    # Available models: gpt-4, gpt-4o, gpt-4-turbo, gpt-4o-mini, gpt-3.5-turbo
    MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4")
    llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
elif MODEL_PROVIDER == "ollama":
    from langchain_ollama import ChatOllama
    # Ollama models (free, local): llama3.1, llama3.2, mistral, qwen2.5, etc.
    MODEL_NAME = os.getenv("OLLAMA_MODEL", "llama3.1")
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    llm = ChatOllama(model=MODEL_NAME, base_url=OLLAMA_BASE_URL, temperature=0)
else:
    raise ValueError(f"Unknown MODEL_PROVIDER: {MODEL_PROVIDER}. Use 'openai' or 'ollama'")

# Where your MCP FastAPI server (server.py) is running
MCP_BASE_URL = os.getenv("MCP_BASE_URL", "http://0.0.0.0:8001")


# ============================================================
# MCP Helper
# ============================================================

def call_mcp_tool(name: str, arguments: Dict[str, Any]) -> Any:
    """
    Call FastAPI MCP tool endpoints defined in server.py.

    server.py exposes:
      - POST /tools/rag.search
      - POST /tools/web.search

    For your current tools:
      - rag.search returns: {"products": [...]}
      - web.search returns: {"results": [...], "note": ...}
    """
    endpoint = f"{MCP_BASE_URL}/tools/{name}"
    print(f"[MCP] Calling {endpoint} with args={arguments}")
    resp = requests.post(endpoint, json=arguments, timeout=20)
    resp.raise_for_status()
    data = resp.json()
    print(f"[MCP] tool={name} response type: {type(data)}")
    return data


# ============================================================
# LangGraph State
# ============================================================

class AgentState(dict):
    input: str
    response: str
    done: bool

    intent: Optional[str]
    plan: Optional[str]
    knowledge: Optional[str]   # text based on MCP results

    retrieved_context: Optional[List[Dict[str, Any]]]


# ============================================================
# Retrieval Tool (MCP-backed)
# ============================================================

def retrieval_tool(query: str) -> str:
    """
    Retrieve information based on a query generated by the Planning/Router agent.

    This calls the MCP server:

      - rag.search: searches your Chroma toys_products vector DB (or stub)
      - web.search: optional live comparison via Serper (shopping/web)

    Parameters
    ----------
    query : str
        Natural language query / plan that describes what to fetch.

    Returns
    -------
    str
        A minimally formatted string summarizing retrieved products
        and (optionally) web prices, to be used as `knowledge`.
    """
    print(f"RETRIEVAL TOOL Invoked with query: {query}")

    # ----- 1) Call rag.search (private catalog) -----
    rag_args: Dict[str, Any] = {
        "query": query,
        "top_k": 5,
        "max_price": None,
        "min_rating": None,
        "brand": None,
    }
    try:
        rag_raw = call_mcp_tool("rag.search", rag_args)
        products: List[Dict[str, Any]] = []
        if isinstance(rag_raw, dict):
            products = rag_raw.get("products", []) or []
    except Exception as e:
        print(f"[retrieval_tool] ERROR calling rag.search: {e}")
        products = []

    # ----- 2) Optionally call web.search (live prices) -----
    try:
        web_args: Dict[str, Any] = {
            "query": query,
            "max_results": 5,
            "mode": "shopping",
        }
        web_raw = call_mcp_tool("web.search", web_args)
        web_results: List[Dict[str, Any]] = []
        if isinstance(web_raw, dict):
            web_results = web_raw.get("results", []) or []
    except Exception as e:
        print(f"[retrieval_tool] ERROR calling web.search: {e}")
        web_results = []

    # ----- 3) Build a simple textual summary for Answer node -----
    lines: List[str] = []

    lines.append("PRIVATE_CATALOG_RESULTS:")
    if not products:
        lines.append("- [no products returned by rag.search]")
    else:
        for i, p in enumerate(products[:3]):   # top-3 only
            lines.append(
                f"- [{i+1}] sku={p.get('sku')} | title={p.get('title')} | "
                f"price={p.get('price')} | rating={p.get('rating')} | "
                f"brand={p.get('brand')} | doc_id={p.get('doc_id')}"
            )

    lines.append("\nWEB_RESULTS:")
    if not web_results:
        lines.append("- [no web results or SERPER_API_KEY not configured]")
    else:
        for i, w in enumerate(web_results[:3]):
            lines.append(
                f"- [{i+1}] title={w.get('title')} | url={w.get('url')} | "
                f"price={w.get('price')} | availability={w.get('availability')} | "
                f"rating={w.get('rating')} (count={w.get('rating_count')})"
            )

    summary = "\n".join(lines)
    print("[retrieval_tool] Summary built for Answer node.")
    return summary


# ============================================================
# Router Node
# ============================================================

def router_node(state: AgentState) -> Dict[str, Any]:
    print("ROUTER NODE Started")
    user_input = state["input"]
    system_prompt = f"""You are the Router Agent. Read the user request and:
1) Identify the main task.
2) Extract constraints (budget, materials, brands).
3) Detect any safety concerns and flag them if necessary.

USER REQUEST:
{user_input}

Your response MUST follow this format:

* Task:
* Constraints:
  - Budget:
  - Material:
  - Brand:
* Safety Flags: (Yes/No — if Yes, provide a brief reason)
"""

    response = llm.invoke(system_prompt)
    print("router response: ", response.content)
    return {"intent": response.content}


# ============================================================
# Planner Node
# ============================================================

def planner_node(state: AgentState) -> Dict[str, Any]:
    print("Planner NODE Started")
    intent = state.get("intent", "No intent provided")
    system_prompt = f"""You are the Planning Agent. Based on the user's intent, you must plan how to retrieve data.

Your responsibilities:
1) Choose the data source(s): **private**, **live**, or **both**
    • Use **both** if the request needs a mix of catalog and live data.
    • Use **private** for catalog-style data (product descriptions, ratings, prices, ingredients).
    • Use **live** for real-time web search or external information not stored in the private catalog.
2) Identify which fields must be retrieved.
3) Determine the comparison criteria used to evaluate options (e.g., price, quality, rating, ingredients, durability).

Intent:
{intent}

Your response MUST follow this format:

* Data Source (private / live / both):
* Fields to Retrieve:
* Comparison Criteria:
"""

    response = llm.invoke(system_prompt)
    print("planner response: ", response.content)
    return {"plan": response.content}


# ============================================================
# Retriever Node (directly calls retrieval_tool → MCP)
# ============================================================

from langchain_core.messages import AIMessage, ToolMessage

def get_final_ai_message(response):
    msgs = response["messages"]
    for msg in reversed(msgs):
        if isinstance(msg, AIMessage):
            return msg
    return None

def get_tool_messages(response):
    msgs = response["messages"]
    return [m for m in msgs if isinstance(m, ToolMessage)]


async def retrieve_node(state: AgentState):
    print("Retriever NODE Started")

    # final_ai, tool_msgs = await async_get_retrieve_result(state)
    client = MultiServerMCPClient(MCP_SERVERS)
    tools = await client.get_tools()

    agent = create_agent(
        model=llm,
        tools=tools
    )

    plan = state.get("plan", "No plan provided")
    query = state.get("input", "No query provided")
    system_prompt = f"""
You are the Retrieval Agent. Your role is to fetch EXACTLY the information specified in the plan—no reasoning, no interpretation.

### Rules
• You MUST retrieve information according to the Data Source specified in the plan.  
• If Data Source includes **private**, use `rag.search` to query the private catalog.  
• If Data Source includes **live**, use `web.search` to query the web.  
• If BOTH sources are listed, call BOTH `rag.search` and `web.search`.  
• Return ONLY the raw or minimally structured data retrieved—do NOT summarize, explain, or modify it.  
• If no data can be retrieved, respond EXACTLY with: **"No data found."**  
• If the plan indicates no retrieval is needed, respond EXACTLY with: **"Retrieval not applicable."**

### User Query
{query}

### Plan Details
{plan}

### Required Response Format
* Retrieved data:
<insert raw retrieved data OR the exact required message>
"""

    response = await agent.ainvoke({"messages": [{"role": "user", "content": system_prompt}]})
    # debug_all(state, system_prompt, response)

    final_ai = get_final_ai_message(response)
    tool_msgs = get_tool_messages(response)

    return {
        "knowledge": final_ai.content if final_ai else None,
        "retrieved_context": [msg.content for msg in tool_msgs]
    }

# ============================================================
# Answer / Critic Node
# ============================================================

def answer_critic_node(state: AgentState) -> Dict[str, Any]:
    print("Answer/Critic NODE Started")
    user_input = state["input"]
    knowledge = state.get("knowledge", "No knowledge provided")
    system_prompt = f"""
You are the Answer Critic Agent. Your job is to synthesize a concise, well-grounded, and safe final answer using ONLY the retrieved knowledge.

### Rules
• Produce a final answer that directly responds to the User Request.  
• You MUST ground all statements in the Retrieved Knowledge — no hallucination, no new facts.  
• Support your answer by citing specific evidence from the retrieved knowledge in bullet points.  
• If any safety risks, harmful content, or missing information appear in the retrieved knowledge, explicitly flag them.  
• If the retrieved knowledge is empty or irrelevant, state: "Insufficient grounded evidence to produce an answer."

### User Request
{user_input}

### Retrieved Knowledge
{knowledge}

### Required Response Format
* Final Answer:
<insert concise, grounded answer with citations and safety warnings as needed>

* Evidence:
  - <retrieved knowledge 1> [source: rag.search]
  - <retrieved knowledge 2> [source: web.search]
  - <retrieved knowledge 3> [source: rag.search]
"""
    
    response = llm.invoke(system_prompt)
    # print("answer response: ", response.content)
    return {"response": response.content, "done": True}


# ============================================================
# Build LangGraph
# ============================================================

graph = StateGraph(AgentState)

graph.add_node("Router", router_node)
graph.add_node("Planner", planner_node)
graph.add_node("Retriver", retrieve_node)
graph.add_node("Answer", answer_critic_node)

graph.add_edge(START, "Router")
graph.add_edge("Router", "Planner")
graph.add_edge("Planner", "Retriver")
graph.add_edge("Retriver", "Answer")

graph.add_conditional_edges(
    "Answer",
    lambda state: "END" if state.get("done") else "Planner",
    {
        "Planner": "Planner",
        "END": END,
    },
)

app = graph.compile()

if __name__ == "__main__":
    result = asyncio.run(
        app.ainvoke({"input": "I want to find a stuffed animal for kids less than $30 in private catalog and live web search"})
    )

    print("\n================ FINAL rag data ===============")
    print(result.get('knowledge'))

    print("\n================ FINAL ANSWER ===============")
    print(result.get("response"))
#     print("============================================\n")
