import asyncio
from langgraph.graph import StateGraph, END, START
from langchain_core.tools import tool
import os
from dotenv import load_dotenv
from typing import Optional, Dict, Any, List
from langchain.agents.factory import create_agent
from langchain_mcp_adapters.client import MultiServerMCPClient
import json
import re

SERVER_HOST = '0.0.0.0'
SERVER_PORT = 8001
SERVER_URL = f'http://{SERVER_HOST}:{SERVER_PORT}'
MCP_SERVERS = {
        "my_server": {
            "url": SERVER_URL+"/mcp",
            "transport": "http",
        },
    }


import requests  # to call MCP HTTP server

# ============================================================
# Environment & LLM
# ============================================================

load_dotenv()

# Model provider selection: "openai" or "ollama" (free)
MODEL_PROVIDER = os.getenv("MODEL_PROVIDER", "openai").lower()

if MODEL_PROVIDER == "openai":
    from langchain_openai import ChatOpenAI
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY is not set in .env file")
    # Model configuration - can be overridden via environment variable
    # Available models: gpt-4, gpt-4o, gpt-4-turbo, gpt-4o-mini, gpt-3.5-turbo
    MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4")
    llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
elif MODEL_PROVIDER == "ollama":
    from langchain_ollama import ChatOllama
    # Ollama models (free, local): llama3.1, llama3.2, mistral, qwen2.5, etc.
    MODEL_NAME = os.getenv("OLLAMA_MODEL", "llama3.1")
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    llm = ChatOllama(model=MODEL_NAME, base_url=OLLAMA_BASE_URL, temperature=0)
else:
    raise ValueError(f"Unknown MODEL_PROVIDER: {MODEL_PROVIDER}. Use 'openai' or 'ollama'")

# Where your MCP FastAPI server (server.py) is running
MCP_BASE_URL = os.getenv("MCP_BASE_URL", "http://0.0.0.0:8001")


# ============================================================
# MCP Helper
# ============================================================

def call_mcp_tool(name: str, arguments: Dict[str, Any]) -> Any:
    """
    Call FastAPI MCP tool endpoints defined in server.py.

    server.py exposes:
      - POST /tools/rag.search
      - POST /tools/web.search

    For your current tools:
      - rag.search returns: {"products": [...]}
      - web.search returns: {"results": [...], "note": ...}
    """
    endpoint = f"{MCP_BASE_URL}/tools/{name}"
    print(f"[MCP] Calling {endpoint} with args={arguments}")
    resp = requests.post(endpoint, json=arguments, timeout=20)
    resp.raise_for_status()
    data = resp.json()
    print(f"[MCP] tool={name} response type: {type(data)}")
    return data


# ============================================================
# LangGraph State
# ============================================================

class AgentState(dict):
    input: str
    response: str
    done: bool

    intent: Optional[str]
    plan: Optional[str]
    knowledge: Optional[str]   # text based on MCP results

    retrieved_context: Optional[List[Dict[str, Any]]]


# ============================================================
# Retrieval Tool (MCP-backed)
# ============================================================

def retrieval_tool(query: str, rag_params: Dict[str, Any] = None) -> str:
    """
    Retrieve information based on a query generated by the Planning/Router agent.

    This calls the MCP server:

      - rag.search: searches your Chroma toys_products vector DB (or stub)
      - web.search: optional live comparison via Serper (shopping/web)

    Parameters
    ----------
    query : str
        Natural language query / plan that describes what to fetch.

    Returns
    -------
    str
        A minimally formatted string summarizing retrieved products
        and (optionally) web prices, to be used as `knowledge`.
    """
    print(f"RETRIEVAL TOOL Invoked with query: {query}")

    # ----- 1) Call rag.search (private catalog) -----
    rag_args: Dict[str, Any] = {
        "query": query,
        "top_k": rag_params.get("top_k", 3) if rag_params else 3,
        "max_price": rag_params.get("max_price") if rag_params else None,
        "min_price": rag_params.get("min_price") if rag_params else None,
        "max_rating": rag_params.get("max_rating") if rag_params else None,
        "min_rating": rag_params.get("min_rating") if rag_params else None,
        "brand": rag_params.get("brand") if rag_params else None,
    }
    try:
        rag_raw = call_mcp_tool("rag.search", rag_args)
        products: List[Dict[str, Any]] = []
        if isinstance(rag_raw, dict):
            products = rag_raw.get("products", []) or []
    except Exception as e:
        print(f"[retrieval_tool] ERROR calling rag.search: {e}")
        products = []

    # ----- 2) Optionally call web.search (live prices) -----
    try:
        web_args: Dict[str, Any] = {
            "query": query,
            "max_results": 5,
            "mode": "shopping",
        }
        web_raw = call_mcp_tool("web.search", web_args)
        web_results: List[Dict[str, Any]] = []
        if isinstance(web_raw, dict):
            web_results = web_raw.get("results", []) or []
    except Exception as e:
        print(f"[retrieval_tool] ERROR calling web.search: {e}")
        web_results = []

    # ----- 3) Build a simple textual summary for Answer node -----
    lines: List[str] = []

    lines.append("PRIVATE_CATALOG_RESULTS:")
    if not products:
        lines.append("- [no products returned by rag.search]")
    else:
        for i, p in enumerate(products[:3]):   # top-3 only
            lines.append(
                f"- [{i+1}] sku={p.get('sku')} | title={p.get('title')} | "
                f"price={p.get('price')} | rating={p.get('rating')} | "
                f"brand={p.get('brand')} | doc_id={p.get('doc_id')}"
            )

    lines.append("\nWEB_RESULTS:")
    if not web_results:
        lines.append("- [no web results or SERPER_API_KEY not configured]")
    else:
        for i, w in enumerate(web_results[:3]):
            lines.append(
                f"- [{i+1}] title={w.get('title')} | url={w.get('url')} | "
                f"price={w.get('price')} | availability={w.get('availability')} | "
                f"rating={w.get('rating')} (count={w.get('rating_count')})"
            )

    summary = "\n".join(lines)
    print("[retrieval_tool] Summary built for Answer node.")
    return summary


# ============================================================
# Router Node
# ============================================================

def router_node(state: AgentState) -> Dict[str, Any]:
    print("ROUTER NODE Started")
    user_input = state["input"]
    system_prompt = f"""You are the Router Agent. Read the user request and:
1) Identify the main task.
2) Extract constraints (budget, materials, brands).
3) Detect any safety concerns and flag them if necessary.

USER REQUEST:
{user_input}

Your response MUST follow this format:

* Task:
* Constraints:
  - Budget:
  - Material:
  - Brand:
* Safety Flags: (Yes/No — if Yes, provide a brief reason)
"""

    response = llm.invoke(system_prompt)
    print("router response: ", response.content)
    return {"intent": response.content}


# ============================================================
# Planner Node
# ============================================================

def planner_node(state: AgentState) -> Dict[str, Any]:
    print("Planner NODE Started")
    intent = state.get("intent", "No intent provided")
    system_prompt = f"""You are the Planning Agent. Based on the user's intent, you must plan how to retrieve data.

Your responsibilities:
1) Choose the data source(s): **private**, **live**, or **both**
    • Use **private** for catalog-style data (product descriptions, ratings, prices, ingredients).
    • Use **live** for real-time or external information not stored in the private catalog.
    • Use **both** if the request needs a mix of catalog and live data.
2) Identify which fields must be retrieved.
3) Determine the comparison criteria used to evaluate options (e.g., price, quality, rating, ingredients, durability).

Intent:
{intent}

Your response MUST follow this format:

* Data Source (private / live / both):
* Fields to Retrieve:
* Comparison Criteria:
  [
    {{
      "attribute": "...",
      "type" ("qualitative" or "quantitative"): "...",
      "value": ...,
      "operator" ("eq", "gt", "lt", "gte", "lte"): "..." (optional if type is "qualitative")
    }},
    ...
  ]
"""

    response = llm.invoke(system_prompt)
    print("planner response: ", response.content)
    return {"plan": response.content}


# ============================================================
# Retriever Node (directly calls retrieval_tool → MCP)
# ============================================================

from langchain_core.messages import AIMessage, ToolMessage

def get_final_ai_message(response):
    msgs = response["messages"]
    for msg in reversed(msgs):
        if isinstance(msg, AIMessage):
            return msg
    return None

def get_tool_messages(response):
    msgs = response["messages"]
    return [m for m in msgs if isinstance(m, ToolMessage)]

def parse_comparison_criteria(plan_text: str) -> List[Dict[str, Any]]:
    """
    Parse comparison criteria from planner's plan text.
    Extracts the JSON array from the "Comparison Criteria:" section.
    
    Returns:
        List of criteria dictionaries with: attribute, type, value, operator
    """
    try:
        # Find the "Comparison Criteria:" section
        criteria_match = re.search(r'\* Comparison Criteria:\s*\n\s*(\[.*?\])', plan_text, re.DOTALL)
        if not criteria_match:
            print("[parse_comparison_criteria] No comparison criteria found in plan")
            return []
        
        criteria_json = criteria_match.group(1)
        # Clean up the JSON string (remove markdown code blocks if any)
        criteria_json = criteria_json.strip()
        if criteria_json.startswith('```'):
            criteria_json = re.sub(r'?\s*', '', criteria_json)
            criteria_json = re.sub(r'```\s*$', '', criteria_json)
        
        criteria_list = json.loads(criteria_json)
        print(f"[parse_comparison_criteria] Parsed {len(criteria_list)} criteria")
        return criteria_list
    except json.JSONDecodeError as e:
        print(f"[parse_comparison_criteria] JSON decode error: {e}")
        return []
    except Exception as e:
        print(f"[parse_comparison_criteria] Error parsing criteria: {e}")
        return []
    
def criteria_to_rag_params(criteria_list: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Convert comparison criteria to rag.search tool parameters.
    
    Mapping:
    - price with lt/lte → max_price
    - price with gt/gte → min_price
    - rating with lte → max_rating
    - rating with gte → min_rating
    
    Returns:
        Dictionary with rag.search parameters
    """
    params: Dict[str, Any] = {}
    
    for criterion in criteria_list:
        if criterion.get("type") != "quantitative":
            continue
        
        attribute = criterion.get("attribute", "").lower()
        operator = criterion.get("operator", "").lower()
        value = criterion.get("value")
        
        if not value:
            continue
        
        # Handle price criteria
        if attribute in ["price", "cost", "budget"]:
            if operator in ["lt", "lte"]:
                # Less than or equal → max_price
                try:
                    price_val = float(value) if isinstance(value, (int, float, str)) else None
                    if price_val is not None:
                        params["max_price"] = price_val
                        print(f"[criteria_to_rag_params] Set max_price={price_val} from {attribute} {operator}")
                except (ValueError, TypeError):
                    pass
            elif operator in ["gt", "gte"]:
                # Greater than or equal → min_price
                try:
                    price_val = float(value) if isinstance(value, (int, float, str)) else None
                    if price_val is not None:
                        params["min_price"] = price_val
                        print(f"[criteria_to_rag_params] Set min_price={price_val} from {attribute} {operator}")
                except (ValueError, TypeError):
                    pass
                
        # Handle rating criteria
        elif attribute in ["rating", "stars", "review_score"]:
            if operator in ["gte", ">="]:
                try:
                    rating_val = float(value) if isinstance(value, (int, float, str)) else None
                    if rating_val is not None:
                        params["min_rating"] = rating_val
                        print(f"[criteria_to_rag_params] Set min_rating={rating_val} from {attribute} {operator}")
                except (ValueError, TypeError):
                    pass
            elif operator in ["lte", "<="]:
                try:
                    rating_val = float(value) if isinstance(value, (int, float, str)) else None
                    if rating_val is not None:
                        params["max_rating"] = rating_val
                        print(f"[criteria_to_rag_params] Set max_rating={rating_val} from {attribute} {operator}")
                except (ValueError, TypeError):
                    pass
        
        # Handle brand criteria
        elif attribute in ["brand", "manufacturer"]:
            if operator == "eq" or not operator:
                if isinstance(value, str):
                    params["brand"] = value
                    print(f"[criteria_to_rag_params] Set brand={value}")
                    
        params["top_k"] = 3
    
    return params

async def async_get_retrieve_result(state: AgentState):
    client = MultiServerMCPClient(MCP_SERVERS)
    tools = await client.get_tools()

    agent = create_agent(
        model=llm,
        tools=tools
    )

    plan = state.get("plan", "No plan provided")
    # Parse comparison criteria from plan
    criteria_list = parse_comparison_criteria(plan)
    rag_params = criteria_to_rag_params(criteria_list)
    
    # Build enhanced system prompt with parsed parameters
    params_description = ""
    if rag_params:
        params_list = []
        if "max_price" in rag_params:
            params_list.append(f"max_price={rag_params['max_price']}")
        if "min_price" in rag_params:
            params_list.append(f"min_price={rag_params['min_price']}")
        if "min_rating" in rag_params:
            params_list.append(f"min_rating={rag_params['min_rating']}")
        if "max_rating" in rag_params:
            params_list.append(f"max_rating={rag_params['max_rating']}")
        if "brand" in rag_params:
            params_list.append(f"brand='{rag_params['brand']}'")
        if "top_k" in rag_params:
            params_list.append(f"top_k={rag_params['top_k']}")
        
        if params_list:
            params_description = f"\n\nIMPORTANT: When calling rag.search, you MUST use these parameters: {', '.join(params_list)}"
            
    system_prompt = f"""You are the Retrieval Agent. Your job is to fetch the information specified in the plan.

Rules:
• You MUST use retrieval_tool() to fetch data whenever retrieval is required by the plan.
• Return ONLY the requested data in raw or minimally structured form (do NOT summarize or interpret it).
• If the requested data cannot be retrieved, respond EXACTLY: "No data found."
• If the plan does not require retrieval, respond EXACTLY: "Retrieval not applicable."

Plan Details:
{plan}
Rag Search/Web Search Parameters:
{params_description}

Your response MUST follow this format:

* Retrieved data:
<insert data here or the exact required message>
"""

    response = await agent.ainvoke({"messages": [{"role": "user", "content": system_prompt}]})

    final_ai = get_final_ai_message(response)
    tool_msgs = get_tool_messages(response)
    
    print("Retriever response: ", response)

    return final_ai, tool_msgs


async def retrieve_node(state: AgentState):
    print("Retriever NODE Started")

    final_ai, tool_msgs = await async_get_retrieve_result(state)
    
    return {
        "knowledge": final_ai.content if final_ai else None,
        "retrieved_context": [msg.content for msg in tool_msgs]
    }

# ============================================================
# Answer / Critic Node
# ============================================================

def answer_critic_node(state: AgentState) -> Dict[str, Any]:
    print("Answer/Critic NODE Started")
    user_input = state["input"]
    knowledge = state.get("knowledge", "No knowledge provided")
    system_prompt = f"""You are the Answer Critic Agent. Your job is to synthesize a concise, cited recommendation; enforce grounding & safety.

Rules:
• Create a final answer that directly addresses the user's request using ONLY the retrieved knowledge.
• Cite specific data points from the retrieved knowledge to support your answer.
• If the retrieved knowledge contains safety concerns, flag them clearly in your answer.

User Request:
{user_input}

Retrieved Knowledge:
{knowledge}

Your response MUST follow this format:
* Final Answer:
<insert final answer with citations and safety notes if applicable>
"""
    response = llm.invoke(system_prompt)
    # print("answer response: ", response.content)
    return {"response": response.content, "done": True}


# ============================================================
# Build LangGraph
# ============================================================

graph = StateGraph(AgentState)

graph.add_node("Router", router_node)
graph.add_node("Planner", planner_node)
graph.add_node("Retriver", retrieve_node)
graph.add_node("Answer", answer_critic_node)

graph.add_edge(START, "Router")
graph.add_edge("Router", "Planner")
graph.add_edge("Planner", "Retriver")
graph.add_edge("Retriver", "Answer")

graph.add_conditional_edges(
    "Answer",
    lambda state: "END" if state.get("done") else "Planner",
    {
        "Planner": "Planner",
        "END": END,
    },
)

app = graph.compile()

# if __name__ == "__main__":
#     result = asyncio.run(
#         app.ainvoke({"input": "I need an eco-friendly stainless-steel cleaner under $15"})
#     )
#     print("\n================ FINAL ANSWER ===============")
#     print(result.get("response"))
#     print("============================================\n")
