from langgraph.graph import StateGraph, END, START
from langchain_core.tools import tool
import os
from dotenv import load_dotenv
from typing import Optional, Dict, Any, List

import requests  # to call MCP HTTP server

# ============================================================
# Environment & LLM
# ============================================================

load_dotenv()

# Model provider selection: "openai" or "ollama" (free)
MODEL_PROVIDER = os.getenv("MODEL_PROVIDER", "openai").lower()

if MODEL_PROVIDER == "openai":
    from langchain_openai import ChatOpenAI
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY is not set in .env file")
    # Model configuration - can be overridden via environment variable
    # Available models: gpt-4, gpt-4o, gpt-4-turbo, gpt-4o-mini, gpt-3.5-turbo
    MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4")
    llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
elif MODEL_PROVIDER == "ollama":
    from langchain_ollama import ChatOllama
    # Ollama models (free, local): llama3.1, llama3.2, mistral, qwen2.5, etc.
    MODEL_NAME = os.getenv("OLLAMA_MODEL", "llama3.1")
    OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    llm = ChatOllama(model=MODEL_NAME, base_url=OLLAMA_BASE_URL, temperature=0)
else:
    raise ValueError(f"Unknown MODEL_PROVIDER: {MODEL_PROVIDER}. Use 'openai' or 'ollama'")

# Where your MCP FastAPI server (server.py) is running
MCP_BASE_URL = os.getenv("MCP_BASE_URL", "http://0.0.0.0:8001")


# ============================================================
# MCP Helper
# ============================================================

def call_mcp_tool(name: str, arguments: Dict[str, Any]) -> Any:
    """
    Call FastAPI MCP tool endpoints defined in server.py.

    server.py exposes:
      - POST /tools/rag.search
      - POST /tools/web.search

    For your current tools:
      - rag.search returns: {"products": [...]}
      - web.search returns: {"results": [...], "note": ...}
    """
    endpoint = f"{MCP_BASE_URL}/tools/{name}"
    print(f"[MCP] Calling {endpoint} with args={arguments}")
    resp = requests.post(endpoint, json=arguments, timeout=20)
    resp.raise_for_status()
    data = resp.json()
    print(f"[MCP] tool={name} response type: {type(data)}")
    return data


# ============================================================
# LangGraph State
# ============================================================

class AgentState(dict):
    input: str
    response: str
    done: bool

    intent: Optional[str]
    plan: Optional[str]
    knowledge: Optional[str]   # text based on MCP results


# ============================================================
# Retrieval Tool (MCP-backed)
# ============================================================

def retrieval_tool(query: str) -> str:
    """
    Retrieve information based on a query generated by the Planning/Router agent.

    This calls the MCP server:

      - rag.search: searches your Chroma toys_products vector DB (or stub)
      - web.search: optional live comparison via Serper (shopping/web)

    Parameters
    ----------
    query : str
        Natural language query / plan that describes what to fetch.

    Returns
    -------
    str
        A minimally formatted string summarizing retrieved products
        and (optionally) web prices, to be used as `knowledge`.
    """
    print(f"RETRIEVAL TOOL Invoked with query: {query}")

    # ----- 1) Call rag.search (private catalog) -----
    rag_args: Dict[str, Any] = {
        "query": query,
        "top_k": 5,
        "max_price": None,
        "min_rating": None,
        "brand": None,
    }
    try:
        rag_raw = call_mcp_tool("rag.search", rag_args)
        products: List[Dict[str, Any]] = []
        if isinstance(rag_raw, dict):
            products = rag_raw.get("products", []) or []
    except Exception as e:
        print(f"[retrieval_tool] ERROR calling rag.search: {e}")
        products = []

    # ----- 2) Optionally call web.search (live prices) -----
    try:
        web_args: Dict[str, Any] = {
            "query": query,
            "max_results": 5,
            "mode": "shopping",
        }
        web_raw = call_mcp_tool("web.search", web_args)
        web_results: List[Dict[str, Any]] = []
        if isinstance(web_raw, dict):
            web_results = web_raw.get("results", []) or []
    except Exception as e:
        print(f"[retrieval_tool] ERROR calling web.search: {e}")
        web_results = []

    # ----- 3) Build a simple textual summary for Answer node -----
    lines: List[str] = []

    lines.append("PRIVATE_CATALOG_RESULTS:")
    if not products:
        lines.append("- [no products returned by rag.search]")
    else:
        for i, p in enumerate(products[:3]):   # top-3 only
            lines.append(
                f"- [{i+1}] sku={p.get('sku')} | title={p.get('title')} | "
                f"price={p.get('price')} | rating={p.get('rating')} | "
                f"brand={p.get('brand')} | doc_id={p.get('doc_id')}"
            )

    lines.append("\nWEB_RESULTS:")
    if not web_results:
        lines.append("- [no web results or SERPER_API_KEY not configured]")
    else:
        for i, w in enumerate(web_results[:3]):
            lines.append(
                f"- [{i+1}] title={w.get('title')} | url={w.get('url')} | "
                f"price={w.get('price')} | availability={w.get('availability')} | "
                f"rating={w.get('rating')} (count={w.get('rating_count')})"
            )

    summary = "\n".join(lines)
    print("[retrieval_tool] Summary built for Answer node.")
    return summary


# ============================================================
# Router Node
# ============================================================

def router_node(state: AgentState) -> Dict[str, Any]:
    print("ROUTER NODE Started")
    user_input = state["input"]
    system_prompt = f"""You are the Router Agent. Read the user request and:
1) Identify the main task.
2) Extract constraints (budget, materials, brands).
3) Detect any safety concerns and flag them if necessary.

USER REQUEST:
{user_input}

Your response MUST follow this format:

* Task:
* Constraints:
  - Budget:
  - Material:
  - Brand:
* Safety Flags: (Yes/No — if Yes, provide a brief reason)
"""

    response = llm.invoke(system_prompt)
    print("router response: ", response.content)
    return {"intent": response.content}


# ============================================================
# Planner Node
# ============================================================

def planner_node(state: AgentState) -> Dict[str, Any]:
    print("Planner NODE Started")
    intent = state.get("intent", "No intent provided")
    system_prompt = f"""You are the Planning Agent. Based on the user's intent, you must plan how to retrieve data.

Your responsibilities:
1) Choose the data source(s): **private**, **live**, or **both**
    • Use **private** for catalog-style data (product descriptions, ratings, prices, ingredients).
    • Use **live** for real-time or external information not stored in the private catalog.
    • Use **both** if the request needs a mix of catalog and live data.
2) Identify which fields must be retrieved.
3) Determine the comparison criteria used to evaluate options (e.g., price, quality, rating, ingredients, durability).

Intent:
{intent}

Your response MUST follow this format:

* Data Source (private / live / both):
* Fields to Retrieve:
* Comparison Criteria:
"""

    response = llm.invoke(system_prompt)
    print("planner response: ", response.content)
    return {"plan": response.content}


# ============================================================
# Retriever Node (directly calls retrieval_tool → MCP)
# ============================================================

def retrieve_node(state: AgentState) -> Dict[str, Any]:
    print("Retriever NODE Started")

    plan = state.get("plan", "No plan provided")

    # Simple: treat the plan as the query for retrieval_tool
    query = f"Retrieve products based on this plan:\n{plan}"
    knowledge_text = retrieval_tool(query)

    print("retrieve response: ", knowledge_text)
    return {"knowledge": knowledge_text}


# ============================================================
# Answer / Critic Node
# ============================================================

def answer_critic_node(state: AgentState) -> Dict[str, Any]:
    print("Answer/Critic NODE Started")
    user_input = state["input"]
    knowledge = state.get("knowledge", "No knowledge provided")
    system_prompt = f"""You are the Answer Critic Agent. Your job is to synthesize a concise, cited recommendation; enforce grounding & safety.

Rules:
• Create a final answer that directly addresses the user's request using ONLY the retrieved knowledge.
• Cite specific data points from the retrieved knowledge to support your answer.
• If the retrieved knowledge contains safety concerns, flag them clearly in your answer.

User Request:
{user_input}

Retrieved Knowledge:
{knowledge}

Your response MUST follow this format:
* Final Answer:
<insert final answer with citations and safety notes if applicable>
"""
    response = llm.invoke(system_prompt)
    # print("answer response: ", response.content)
    return {"response": response.content, "done": True}


# ============================================================
# Build LangGraph
# ============================================================

graph = StateGraph(AgentState)

graph.add_node("Router", router_node)
graph.add_node("Planner", planner_node)
graph.add_node("Retriver", retrieve_node)
graph.add_node("Answer", answer_critic_node)

graph.add_edge(START, "Router")
graph.add_edge("Router", "Planner")
graph.add_edge("Planner", "Retriver")
graph.add_edge("Retriver", "Answer")

graph.add_conditional_edges(
    "Answer",
    lambda state: "END" if state.get("done") else "Planner",
    {
        "Planner": "Planner",
        "END": END,
    },
)

app = graph.compile()

if __name__ == "__main__":
    result = app.invoke(
        {"input": "I need an eco-friendly stainless-steel cleaner under $15"}
    )
    print("\n================ FINAL ANSWER ===============")
    print(result.get("response"))
    print("============================================\n")
